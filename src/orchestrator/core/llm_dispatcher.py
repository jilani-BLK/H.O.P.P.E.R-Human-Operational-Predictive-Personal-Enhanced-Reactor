"""
HOPPER - LLM Dispatcher
G√®re le routing intelligent vers LLM pour conversations naturelles
Phase 2: Int√©gration prompts et contexte
"""

from typing import Dict, Any, Optional, List
import requests
from loguru import logger


class LLMDispatcher:
    """
    Dispatcher intelligent pour conversations LLM
    G√®re templates de prompts et contexte
    """
    
    # Template syst√®me pour HOPPER
    SYSTEM_PROMPT = """Tu es HOPPER, un assistant personnel intelligent et local fonctionnant enti√®rement hors ligne sur macOS.

Tes capacit√©s:
- R√©pondre aux questions en fran√ßais
- Ex√©cuter des commandes syst√®me (liste fichiers, cr√©er fichiers, ouvrir applications)
- Acc√©der √† une base de connaissances locale
- Maintenir une conversation naturelle

Personnalit√©:
- Professionnel mais chaleureux
- Concis et pr√©cis
- Toujours en fran√ßais
- Tu te pr√©sentes comme "HOPPER" si on te demande

R√©ponds de mani√®re claire et directe."""

    # Mots-cl√©s pour commandes conversationnelles (priorit√© haute - toujours LLM)
    CONVERSATION_KEYWORDS = [
        "apprends", "retiens", "retenir", "souviens", "savoir",
        "qui es-tu", "qui es tu", "pr√©sente-toi", "pr√©sente toi",
        "explique", "raconte", "parle-moi", "parle moi",
        "penses-tu", "penses tu", "qu'est-ce que", "quest-ce que",
        "comment √ßa marche", "pourquoi", "comment", "dis-moi"
    ]
    
    # Mots-cl√©s indiquant une commande syst√®me (pas LLM)
    SYSTEM_KEYWORDS = [
        "liste", "affiche", "montre", "voir",
        "cr√©e", "cr√©er", "nouveau", "touch",
        "ouvre", "ouvrir", "lance", "lancer",
        "lis", "lire", "cat", "contenu",
        "date", "heure", "pwd"
    ]
    
    def __init__(self, llm_service_url: str = "http://llm:5001"):
        """
        Initialise le dispatcher LLM
        
        Args:
            llm_service_url: URL du service LLM
        """
        self.llm_url = llm_service_url
        logger.info(f"üéØ LLMDispatcher initialis√© (Phase 2) -> {llm_service_url}")
    
    def is_system_command(self, text: str) -> bool:
        """
        D√©tecte si le texte est une commande syst√®me
        
        Args:
            text: Texte √† analyser
            
        Returns:
            True si c'est une commande syst√®me
        """
        text_lower = text.lower()
        
        # PRIORIT√â 1: V√©rifier d'abord si c'est conversationnel (pas syst√®me)
        # Si contient mot-cl√© conversation, c'est TOUJOURS LLM
        for keyword in self.CONVERSATION_KEYWORDS:
            if keyword in text_lower:
                logger.debug(f"üó£Ô∏è Conversation d√©tect√©e ('{keyword}'): {text[:50]}")
                return False  # Pas syst√®me, c'est conversationnel
        
        # PRIORIT√â 2: V√©rifier mots-cl√©s syst√®me
        for keyword in self.SYSTEM_KEYWORDS:
            if keyword in text_lower:
                logger.debug(f"‚öôÔ∏è Syst√®me d√©tect√© ('{keyword}'): {text[:50]}")
                return True
        
        # PRIORIT√â 3: Heuristique - court + "fichier/dossier"
        words = text_lower.split()
        if len(words) < 8 and ("fichier" in text_lower or "dossier" in text_lower):
            logger.debug(f"üìÅ Syst√®me d√©tect√© (heuristique fichier): {text[:50]}")
            return True
        
        # Par d√©faut: conversationnel
        logger.debug(f"üí¨ Conversationnel par d√©faut: {text[:50]}")
        return False
    
    def build_prompt(
        self,
        user_message: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        context: Optional[str] = None
    ) -> str:
        """
        Construit le prompt complet pour le LLM
        
        Args:
            user_message: Message de l'utilisateur
            conversation_history: Historique des √©changes
            context: Contexte additionnel (KB, etc.)
            
        Returns:
            Prompt format√©
        """
        parts = [self.SYSTEM_PROMPT, ""]
        
        # Ajouter contexte de la base de connaissances si disponible
        if context:
            parts.append(f"Contexte pertinent:\n{context}\n")
        
        # Ajouter historique de conversation
        if conversation_history:
            parts.append("Historique de la conversation:")
            for exchange in conversation_history[-5:]:  # Limiter √† 5 derniers √©changes
                role = exchange.get("role", "user")
                content = exchange.get("content", "")
                if role == "user":
                    parts.append(f"Utilisateur: {content}")
                elif role == "assistant":
                    parts.append(f"HOPPER: {content}")
            parts.append("")
        
        # Ajouter le message actuel
        parts.append(f"Utilisateur: {user_message}")
        parts.append("HOPPER:")
        
        return "\n".join(parts)
    
    def generate(
        self,
        user_message: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        context: Optional[str] = None,
        max_tokens: int = 300,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """
        G√©n√®re une r√©ponse via le LLM
        
        Args:
            user_message: Message utilisateur
            conversation_history: Historique conversation
            context: Contexte additionnel
            max_tokens: Nombre max de tokens
            temperature: Temp√©rature de g√©n√©ration
            
        Returns:
            R√©ponse du LLM
        """
        logger.info(f"üìù G√©n√©ration r√©ponse LLM: {user_message[:50]}...")
        
        # Construire le prompt
        prompt = self.build_prompt(user_message, conversation_history, context)
        
        logger.debug(f"Prompt: {len(prompt)} chars")
        
        try:
            # Appeler le service LLM
            response = requests.post(
                f"{self.llm_url}/generate",
                json={
                    "prompt": prompt,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "stop": ["Utilisateur:", "\nUtilisateur:", "User:"]
                },
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                answer = data.get("text", "").strip()
                
                logger.success(f"‚úÖ R√©ponse g√©n√©r√©e: {len(answer)} chars")
                
                return {
                    "success": True,
                    "response": answer,
                    "tokens": data.get("tokens_generated", 0),
                    "model": data.get("model", "unknown")
                }
            else:
                logger.error(f"‚ùå Erreur LLM: {response.status_code}")
                return {
                    "success": False,
                    "error": f"LLM error: {response.status_code}",
                    "response": "D√©sol√©, je rencontre un probl√®me technique."
                }
                
        except Exception as e:
            logger.error(f"‚ùå Exception LLM: {e}")
            return {
                "success": False,
                "error": str(e),
                "response": "D√©sol√©, je ne peux pas r√©pondre pour le moment."
            }
    
    def route(self, command: str) -> Dict[str, Any]:
        """
        Route la commande vers le bon dispatcher
        
        Args:
            command: Commande/question utilisateur
            
        Returns:
            R√©sultat du routing avec type (system/llm)
        """
        if self.is_system_command(command):
            return {
                "type": "system",
                "reason": "detected_system_keywords"
            }
        else:
            return {
                "type": "llm",
                "reason": "conversational_query"
            }


# Test standalone
if __name__ == "__main__":
    dispatcher = LLMDispatcher("http://localhost:5001")
    
    test_cases = [
        "liste les fichiers du dossier /tmp",
        "comment vas-tu aujourd'hui ?",
        "cr√©e un fichier test.txt",
        "qu'est-ce que tu penses de l'intelligence artificielle ?",
        "ouvre l'application Calculator",
        "explique-moi ce qu'est un LLM"
    ]
    
    print("\nüß™ Test du routing:\n")
    for test in test_cases:
        result = dispatcher.route(test)
        print(f"üìù '{test}'")
        print(f"   ‚Üí Type: {result['type']} ({result['reason']})")
        print()
