"""
Relevance Engine - Filtrage Intelligent d'√âv√©nements

D√©termine quels √©v√©nements m√©ritent d'√™tre annonc√©s √† l'utilisateur
via scoring multi-crit√®res (r√®gles + LLM + pr√©f√©rences).
"""

import asyncio
import json
from typing import Dict, Any, Optional, List
from enum import Enum
from dataclasses import dataclass, asdict
from datetime import datetime
from loguru import logger

from .models import PerceptionEvent, RiskLevel


class RelevanceScore(str, Enum):
    """Scores de pertinence d'un √©v√©nement"""
    CRITICAL = "critical"      # Annonce imm√©diate obligatoire
    HIGH = "high"              # Annonce imm√©diate recommand√©e
    MEDIUM = "medium"          # Annonce diff√©r√©e/group√©e
    LOW = "low"                # Log uniquement, pas d'annonce
    NOISE = "noise"            # Ignor√© compl√®tement


@dataclass
class ScoredEvent:
    """√âv√©nement avec score de pertinence"""
    event: PerceptionEvent
    relevance_score: RelevanceScore
    score_value: float  # 0.0-1.0
    reasoning: str
    should_announce: bool
    priority: int  # 1-10 (10 = urgent)
    scored_at: str
    

class RelevanceEngine:
    """
    Moteur de pertinence pour filtrage intelligent d'√©v√©nements
    
    Pipeline:
    1. R√®gles heuristiques (rapide, 90% des cas)
    2. Scoring LLM (pour cas ambigus)
    3. Pr√©f√©rences utilisateur (overrides)
    4. Gestion de d√©duplication/rate limiting
    """
    
    def __init__(
        self,
        llm_service_url: str,
        user_preferences: Optional[Dict[str, Any]] = None,
        rate_limit_window: int = 300  # 5 minutes
    ):
        self.llm_service_url = llm_service_url
        self.user_preferences = user_preferences or {}
        self.rate_limit_window = rate_limit_window
        
        # Cache pour rate limiting
        self._recent_announcements: List[Dict[str, Any]] = []
        
        # Seuils par d√©faut
        self.thresholds = {
            "email_important_score": 0.7,
            "security_threat_level": "MEDIUM",
            "system_cpu_percent": 90,
            "max_announcements_per_hour": 10,
            "deduplicate_window_seconds": 60
        }
        
        logger.info("‚úÖ RelevanceEngine initialis√©")
    
    
    async def score_event(self, event: PerceptionEvent) -> ScoredEvent:
        """
        Score un √©v√©nement pour d√©terminer sa pertinence
        
        Args:
            event: √âv√©nement √† scorer
            
        Returns:
            ScoredEvent avec score et d√©cision d'annonce
        """
        
        # 1. R√®gles heuristiques rapides
        heuristic_result = self._apply_heuristic_rules(event)
        
        if heuristic_result["confident"]:
            # R√®gle heuristique est s√ªre, pas besoin du LLM
            return ScoredEvent(
                event=event,
                relevance_score=heuristic_result["score"],
                score_value=heuristic_result["value"],
                reasoning=heuristic_result["reasoning"],
                should_announce=heuristic_result["announce"],
                priority=heuristic_result["priority"],
                scored_at=datetime.now().isoformat()
            )
        
        # 2. Cas ambigu ‚Üí demander au LLM
        llm_result = await self._score_with_llm(event)
        
        if llm_result:
            return ScoredEvent(
                event=event,
                relevance_score=llm_result["score"],
                score_value=llm_result["value"],
                reasoning=llm_result["reasoning"],
                should_announce=llm_result["announce"],
                priority=llm_result["priority"],
                scored_at=datetime.now().isoformat()
            )
        
        # 3. Fallback: score neutre
        return ScoredEvent(
            event=event,
            relevance_score=RelevanceScore.LOW,
            score_value=0.3,
            reasoning="Scoring fallback - √©v√©nement non reconnu",
            should_announce=False,
            priority=3,
            scored_at=datetime.now().isoformat()
        )
    
    
    def _apply_heuristic_rules(self, event: PerceptionEvent) -> Dict[str, Any]:
        """
        Applique r√®gles heuristiques rapides
        
        Returns:
            {
                "confident": bool,  # La r√®gle est-elle s√ªre?
                "score": RelevanceScore,
                "value": float,
                "reasoning": str,
                "announce": bool,
                "priority": int
            }
        """
        
        source = event.source
        event_type = event.event_type
        data = event.data
        
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # üîê S√âCURIT√â - Toujours critique
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if source == "malware_detector":
            threat_level = data.get("threat_level", "MEDIUM")
            
            if threat_level in ["HIGH", "CRITICAL"]:
                return {
                    "confident": True,
                    "score": RelevanceScore.CRITICAL,
                    "value": 1.0,
                    "reasoning": f"Menace de s√©curit√© d√©tect√©e: {threat_level}",
                    "announce": True,
                    "priority": 10
                }
            elif threat_level == "MEDIUM":
                return {
                    "confident": True,
                    "score": RelevanceScore.HIGH,
                    "value": 0.8,
                    "reasoning": "Menace potentielle d√©tect√©e",
                    "announce": True,
                    "priority": 7
                }
        
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # üìß EMAIL - Bas√© sur importance et exp√©diteur
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if source == "email_connector" and event_type == "new_email":
            importance = data.get("importance", "normal")
            sender = data.get("sender", "")
            is_vip = self._is_vip_sender(sender)
            
            if importance == "high" or is_vip:
                return {
                    "confident": True,
                    "score": RelevanceScore.HIGH,
                    "value": 0.85,
                    "reasoning": f"Email important de {sender}",
                    "announce": True,
                    "priority": 8
                }
            elif importance == "normal":
                # Cas ambigu ‚Üí demander au LLM d'analyser le sujet
                return {
                    "confident": False,
                    "score": RelevanceScore.MEDIUM,
                    "value": 0.5,
                    "reasoning": "Email standard, analyse LLM requise",
                    "announce": False,
                    "priority": 5
                }
        
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # ‚öôÔ∏è SYST√àME - Selon criticit√© ressources
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if source == "system_executor" and event_type == "resource_alert":
            cpu_percent = data.get("cpu_percent", 0)
            memory_percent = data.get("memory_percent", 0)
            
            if cpu_percent > 95 or memory_percent > 95:
                return {
                    "confident": True,
                    "score": RelevanceScore.HIGH,
                    "value": 0.9,
                    "reasoning": f"Ressources critiques: CPU {cpu_percent}%, RAM {memory_percent}%",
                    "announce": True,
                    "priority": 9
                }
            elif cpu_percent > 80 or memory_percent > 80:
                return {
                    "confident": True,
                    "score": RelevanceScore.MEDIUM,
                    "value": 0.6,
                    "reasoning": "Ressources √©lev√©es mais g√©rables",
                    "announce": False,
                    "priority": 5
                }
        
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # üìÅ FICHIERS - Modifications importantes uniquement
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if source == "filesystem_tools":
            if event_type == "file_deleted":
                path = data.get("path", "")
                
                # Documents importants
                if any(ext in path for ext in [".doc", ".pdf", ".key", ".xls"]):
                    return {
                        "confident": True,
                        "score": RelevanceScore.MEDIUM,
                        "value": 0.6,
                        "reasoning": f"Document supprim√©: {path}",
                        "announce": True,
                        "priority": 6
                    }
                else:
                    return {
                        "confident": True,
                        "score": RelevanceScore.LOW,
                        "value": 0.2,
                        "reasoning": "Fichier temporaire supprim√©",
                        "announce": False,
                        "priority": 2
                    }
        
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # ü§∑ INCONNU - Passer au LLM
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        return {
            "confident": False,
            "score": RelevanceScore.LOW,
            "value": 0.3,
            "reasoning": "√âv√©nement non classifi√© par r√®gles heuristiques",
            "announce": False,
            "priority": 3
        }
    
    
    async def _score_with_llm(self, event: PerceptionEvent) -> Optional[Dict[str, Any]]:
        """
        Score un √©v√©nement via LLM pour cas ambigus
        
        Returns:
            Dict avec score, reasoning, announce, priority ou None si √©chec
        """
        
        try:
            import aiohttp
            
            # Construire prompt de scoring
            prompt = self._build_scoring_prompt(event)
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.llm_service_url}/generate",
                    json={
                        "prompt": prompt,
                        "max_tokens": 150,
                        "temperature": 0.3,  # Faible pour consistance
                        "stop": ["\n\n"]
                    },
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status != 200:
                        logger.warning(f"LLM scoring √©chec: {response.status}")
                        return None
                    
                    result = await response.json()
                    llm_text = result.get("text", "").strip()
                    
                    # Parser r√©ponse LLM (format attendu: JSON)
                    return self._parse_llm_scoring(llm_text)
        
        except Exception as e:
            logger.error(f"Erreur scoring LLM: {e}")
            return None
    
    
    def _build_scoring_prompt(self, event: PerceptionEvent) -> str:
        """Construit prompt pour scoring LLM"""
        
        return f"""Tu es un assistant qui √©value la pertinence d'√©v√©nements syst√®me.

√âv√©nement:
- Source: {event.source}
- Type: {event.event_type}
- Priorit√©: {event.priority}
- Donn√©es: {json.dumps(event.data, indent=2)}

√âvalue cet √©v√©nement selon ces crit√®res:
1. Criticit√©: N√©cessite-t-il une action imm√©diate?
2. Importance: Est-ce que l'utilisateur veut √™tre inform√©?
3. Urgence: Peut-on attendre ou faut-il interrompre?

R√©ponds en JSON:
{{
  "score": "critical|high|medium|low|noise",
  "value": 0.0-1.0,
  "reasoning": "Explication courte",
  "announce": true|false,
  "priority": 1-10
}}

JSON:"""
    
    
    def _parse_llm_scoring(self, llm_text: str) -> Optional[Dict[str, Any]]:
        """Parse r√©ponse JSON du LLM"""
        
        try:
            # Extraire JSON
            json_start = llm_text.find('{')
            json_end = llm_text.rfind('}') + 1
            
            if json_start == -1:
                return None
            
            json_str = llm_text[json_start:json_end]
            data = json.loads(json_str)
            
            # Valider structure
            score_str = data.get("score", "low")
            score = RelevanceScore(score_str) if score_str in RelevanceScore.__members__.values() else RelevanceScore.LOW
            
            return {
                "score": score,
                "value": float(data.get("value", 0.3)),
                "reasoning": data.get("reasoning", "LLM scoring"),
                "announce": bool(data.get("announce", False)),
                "priority": int(data.get("priority", 3))
            }
        
        except Exception as e:
            logger.error(f"Parse LLM scoring √©chec: {e}")
            return None
    
    
    def should_rate_limit(self, scored_event: ScoredEvent) -> bool:
        """
        V√©rifie si l'√©v√©nement doit √™tre rate-limited
        
        Args:
            scored_event: √âv√©nement scor√©
            
        Returns:
            True si doit √™tre bloqu√© par rate limiting
        """
        
        now = datetime.now()
        
        # Nettoyer les anciennes annonces (> 5 minutes)
        self._recent_announcements = [
            ann for ann in self._recent_announcements
            if (now - datetime.fromisoformat(ann["timestamp"])).total_seconds() < self.rate_limit_window
        ]
        
        # D√©duplication: m√™me source/type dans derni√®re minute
        for ann in self._recent_announcements:
            if (
                ann["source"] == scored_event.event.source and
                ann["event_type"] == scored_event.event.event_type and
                (now - datetime.fromisoformat(ann["timestamp"])).total_seconds() < 60
            ):
                logger.debug(f"‚è∏Ô∏è  Rate-limited (d√©dupliqu√©): {scored_event.event.source}/{scored_event.event.event_type}")
                return True
        
        # Limite d'annonces par fen√™tre
        max_per_window = self.thresholds.get("max_announcements_per_hour", 10)
        if len(self._recent_announcements) >= max_per_window:
            logger.warning(f"‚è∏Ô∏è  Rate-limited (max {max_per_window} annonces/5min)")
            return True
        
        # Enregistrer cette annonce
        self._recent_announcements.append({
            "source": scored_event.event.source,
            "event_type": scored_event.event.event_type,
            "timestamp": now.isoformat()
        })
        
        return False
    
    
    def _is_vip_sender(self, sender: str) -> bool:
        """V√©rifie si l'exp√©diteur est VIP selon pr√©f√©rences utilisateur"""
        
        vip_senders = self.user_preferences.get("vip_email_senders", [])
        
        # V√©rifier domaines VIP aussi
        vip_domains = self.user_preferences.get("vip_email_domains", [])
        sender_domain = sender.split("@")[-1] if "@" in sender else ""
        
        return sender in vip_senders or sender_domain in vip_domains
    
    
    def update_preferences(self, preferences: Dict[str, Any]):
        """Met √† jour les pr√©f√©rences utilisateur"""
        self.user_preferences.update(preferences)
        logger.info(f"‚úÖ Pr√©f√©rences mises √† jour: {list(preferences.keys())}")
