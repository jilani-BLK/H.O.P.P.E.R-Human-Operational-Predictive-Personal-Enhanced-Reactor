"""
Dispatcher d'intentions
Analyse les commandes et les route vers les services appropri√©s
Phase 2: Int√©gration LLM complet avec RAG et PromptBuilder
"""

import re
from typing import Dict, Any, Optional
from loguru import logger

from core.service_registry import ServiceRegistry
from core.context_manager import ContextManager
from core.prompt_builder import PromptBuilder

# Import ActionNarrator pour communication transparente
try:
    from src.communication import ActionNarrator, narrate_system_command, ActionType, Action, Urgency
    logger.info("‚úÖ ActionNarrator import√©")
except ImportError:
    logger.warning("‚ö†Ô∏è ActionNarrator non disponible")
    ActionNarrator = None
    narrate_system_command = None

try:
    from ..config import settings
except ImportError:
    from config import settings  # type: ignore[import-not-found]


class IntentDispatcher:
    """Route les commandes vers les services appropri√©s"""
    
    def __init__(self, service_registry: ServiceRegistry, context_manager: ContextManager):
        self.service_registry = service_registry
        self.context_manager = context_manager
        
        # Initialiser ActionNarrator pour communication transparente
        if ActionNarrator:
            self.narrator = ActionNarrator(verbose=True, auto_approve_low_risk=True)
            logger.info("‚úÖ ActionNarrator initialis√© - Communication transparente activ√©e")
        else:
            self.narrator = None
            logger.warning("‚ö†Ô∏è ActionNarrator non disponible")
        
        # Initialiser PromptBuilder pour Phase 2
        try:
            self.prompt_builder = PromptBuilder()
            logger.info("‚úÖ PromptBuilder initialis√©")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è PromptBuilder non disponible: {e}")
            self.prompt_builder = None
        
        # Initialiser System Tools (Phase 5)
        try:
            from tools.system_integration import system_tools
            from tools.filesystem_integration import fs_tools
            self.system_tools = system_tools
            self.fs_tools = fs_tools
            logger.info("‚úÖ System Tools int√©gr√©s (LocalSystem + FileSystem)")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è System Tools non disponibles: {e}")
            self.system_tools = None
            self.fs_tools = None
        
        # Patterns d'intentions simples (Phase 1)
        self.intent_patterns = {
            "system_action": [
                r"\b(ouvre|ouvrir|lance|lancer|d√©marre|d√©marrer)\b.*\b(fichier|application|app|programme)\b",
                r"\b(cr√©e|cr√©er|nouveau)\b.*\bfichier\b",
                r"\b(supprime|supprimer|efface|effacer)\b.*\bfichier\b",
                r"\b(liste|lister|affiche|afficher)\b.*\b(fichiers|r√©pertoire|dossier)\b"
            ],
            "learn": [
                r"\b(apprends?|retiens?|m√©morise|note)\b",
                r"\b(learn|remember)\b"
            ],
            "question": [
                r"^(quel|quelle|quels|quelles|qui|quoi|o√π|comment|pourquoi|combien)",
                r"\?$"
            ],
            "email": [
                r"\b(email|mail|message|courrier)\b",
                r"\b(inbox|bo√Æte de r√©ception)\b"
            ],
            "control": [
                r"\b(√©teins|allume|active|d√©sactive)\b.*\b(lumi√®re|lampe)\b",
                r"\b(volume|son)\b"
            ]
        }
    
    async def dispatch(
        self,
        text: str,
        user_id: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Route une commande vers le bon service
        
        Args:
            text: Texte de la commande
            user_id: Identifiant utilisateur
            context: Contexte actuel
            
        Returns:
            R√©sultat de l'ex√©cution
        """
        logger.info(f"üîç Analyse de l'intention pour: '{text}'")
        
        # D√©tection de l'intention
        intent = self._detect_intent(text)
        logger.info(f"üí° Intention d√©tect√©e: {intent}")
        
        # Routage selon l'intention
        if intent == "system_action":
            return await self._handle_system_action(text, user_id, context)
        
        elif intent == "learn":
            return await self._handle_learn(text, user_id, context)
        
        elif intent == "question":
            return await self._handle_question(text, user_id, context)
        
        elif intent == "email":
            return await self._handle_email(text, user_id, context)
        
        elif intent == "control":
            return await self._handle_control(text, user_id, context)
        
        else:
            # Par d√©faut, envoyer au LLM pour traitement g√©n√©ral
            return await self._handle_general(text, user_id, context)
    
    def _detect_intent(self, text: str) -> str:
        """
        D√©tecte l'intention d'une commande
        
        Args:
            text: Texte √† analyser
            
        Returns:
            Type d'intention d√©tect√©e
        """
        text_lower = text.lower()
        
        for intent, patterns in self.intent_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    return intent
        
        return "general"
    
    async def _generate_action_narration(self, text: str, action_type: str) -> str:
        """
        G√©n√®re une narration de ce que HOPPER est en train de faire
        
        Args:
            text: Commande utilisateur
            action_type: Type d'action (system_action, learn, control, etc.)
            
        Returns:
            Phrase d√©crivant l'action en cours
        """
        logger.debug(f"üé¨ G√©n√©ration narration pour: {text} (type: {action_type})")
        
        # Templates de narration selon le type d'action
        narration_templates = {
            "system_action": "Je m'occupe de {action}...",
            "learn": "Je m√©morise cette information...",
            "control": "J'effectue le contr√¥le demand√©...",
            "email": "Je consulte vos emails...",
            "search": "Je recherche l'information..."
        }
        
        # Utiliser le LLM pour g√©n√©rer une narration naturelle
        try:
            logger.debug("ü§ñ Appel LLM pour narration...")
            prompt = f"""Tu es HOPPER, un assistant. L'utilisateur te demande: "{text}"

R√©ponds en UNE SEULE phrase courte (maximum 15 mots) pour dire ce que tu es EN TRAIN DE FAIRE.
Commence par "Je" et utilise le pr√©sent continu.

Exemples:
- Pour "ouvre mon fichier test.txt" ‚Üí "J'ouvre le fichier test.txt pour vous"
- Pour "cr√©e un dossier projets" ‚Üí "Je cr√©e le dossier projets"
- Pour "liste les fichiers" ‚Üí "Je liste les fichiers du r√©pertoire"
- Pour "apprends que Paris est la capitale" ‚Üí "Je m√©morise cette information"
- Pour "allume la lumi√®re du salon" ‚Üí "J'allume la lumi√®re du salon"

Ta r√©ponse (une seule phrase, maximum 15 mots):"""

            result = await self.service_registry.call_service(
                "llm",
                "/generate",
                method="POST",
                data={
                    "prompt": prompt,
                    "max_tokens": 50,
                    "temperature": 0.3  # Basse temp√©rature pour r√©ponses coh√©rentes
                }
            )
            
            narration = result.get("text", "").strip()
            logger.debug(f"‚ú® Narration g√©n√©r√©e: {narration}")
            if narration and len(narration) < 200:  # V√©rification s√©curit√©
                return narration
                
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Fallback template (LLM indispo): {e}")
        
        # Fallback : template simple
        template = narration_templates.get(action_type, "Je traite votre demande...")
        fallback = template.format(action=text.lower()[:50])
        logger.debug(f"üìù Template utilis√©: {fallback}")
        return fallback
    
    async def _handle_system_action(
        self,
        text: str,
        user_id: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """G√®re les actions syst√®me avec narration transparente"""
        logger.info("‚öôÔ∏è Traitement d'une action syst√®me")
        
        # Narration transparente avec ActionNarrator
        if self.narrator and narrate_system_command:
            # Cr√©er action d√©taill√©e
            action = Action(
                action_type=ActionType.SYSTEM_COMMAND,
                description=f"Ex√©cuter : {text}",
                reason="traiter votre demande",
                estimated_duration="quelques secondes",
                urgency=Urgency.MEDIUM,
                requires_approval=False,  # Peut √™tre True selon la commande
                benefits=["Ex√©cution de votre commande"],
            )
            
            # Narrer l'action
            approved = self.narrator.narrate(action)
            
            if not approved:
                return {
                    "message": "Action annul√©e par l'utilisateur",
                    "data": None,
                    "actions": ["cancelled"]
                }
        else:
            # Fallback : G√©n√©ration simple
            narration = await self._generate_action_narration(text, "system_action")
        
        try:
            # Appel au module d'ex√©cution syst√®me
            result = await self.service_registry.call_service(
                "system_executor",
                "/exec",
                method="POST",
                data={"command": text, "args": [], "timeout": 30}
            )
            
            # Message de succ√®s
            success_msg = f"‚úÖ Action termin√©e avec succ√®s"
            
            return {
                "message": success_msg,
                "data": result,
                "actions": ["system_execution"],
                "narration": action.description if self.narrator else narration
            }
            
        except Exception as e:
            logger.error(f"Erreur d'ex√©cution syst√®me: {str(e)}")
            return {
                "message": f"‚ùå Erreur lors de l'ex√©cution: {str(e)}",
                "data": None,
                "actions": []
            }
    
    async def _handle_question(
        self,
        text: str,
        user_id: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        G√®re les questions n√©cessitant le LLM avec RAG (Retrieval-Augmented Generation)
        Phase 2: Utilise PromptBuilder et enrichit avec Knowledge Base
        Phase 5: D√©tecte et ex√©cute automatiquement les outils syst√®me
        """
        logger.info("ü§ñ Traitement d'une question via LLM + RAG + System Tools")
        
        try:
            # 1. Enrichir avec Knowledge Base (RAG)
            knowledge_context = await self._enrich_with_knowledge(text)
            
            # 2. R√©cup√©rer historique conversationnel
            history = self.context_manager.get_history_for_prompt(user_id, max_exchanges=5)
            
            # 3. Construire prompt avec PromptBuilder
            if self.prompt_builder:
                prompt = self.prompt_builder.build_prompt(
                    user_input=text,
                    history=history,
                    knowledge_context=knowledge_context
                )
                generation_params = self.prompt_builder.get_generation_params()
            else:
                # Fallback si PromptBuilder indisponible
                logger.warning("‚ö†Ô∏è PromptBuilder indisponible, utilisation prompt simple")
                history_text = self.context_manager.format_history_for_llm(user_id)
                prompt = f"{history_text}\n\nUser: {text}\nAssistant:"
                generation_params: Dict[str, Any] = {"max_tokens": 500, "temperature": 0.7}
            
            # 4. Appel au LLM
            result = await self.service_registry.call_service(
                "llm",
                "/generate",
                method="POST",
                data={
                    "prompt": prompt,
                    **generation_params
                },
                timeout=settings.LLM_TIMEOUT
            )
            
            # 5. Extraire r√©ponse
            response_text = result.get("text", result.get("response", ""))
            
            # 6. PHASE 5: D√©tecter et ex√©cuter outils syst√®me
            tool_results = []
            actions_executed = ["llm_generation"]
            
            if self.system_tools:
                try:
                    # D√©tecter dans la r√©ponse LLM et la question utilisateur
                    tool_result = await self.system_tools.detect_and_execute(response_text, text)
                    if tool_result:
                        logger.success(f"üîß Outil ex√©cut√©: {tool_result['action']}")
                        tool_results.append(tool_result)
                        actions_executed.append(f"tool_{tool_result['action']}")
                        
                        # Enrichir la r√©ponse avec le r√©sultat
                        tool_context = self.system_tools.format_result_for_llm(tool_result)
                        response_text += tool_context
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erreur ex√©cution outil syst√®me: {e}")
            
            # 7. Sauvegarder dans historique
            self.context_manager.add_to_history(user_id, text, response_text)
            
            logger.success(f"‚úÖ R√©ponse LLM g√©n√©r√©e: {result.get('tokens_generated', 0)} tokens, {len(tool_results)} outils ex√©cut√©s")
            
            return {
                "message": response_text,
                "data": result,
                "tools_executed": tool_results,
                "actions": actions_executed + (["rag_enrichment"] if knowledge_context else [])
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur LLM: {str(e)}")
            return {
                "message": f"D√©sol√©, je n'ai pas pu traiter votre question: {str(e)}",
                "data": None,
                "actions": []
            }
    
    async def _enrich_with_knowledge(self, query: str) -> Optional[str]:
        """
        Enrichit le prompt avec la Knowledge Base (RAG)
        
        Args:
            query: Requ√™te utilisateur
            
        Returns:
            Contexte pertinent ou None
        """
        try:
            # Rechercher dans KB
            result = await self.service_registry.call_service(
                "llm",
                "/search",
                method="POST",
                data={"query": query, "k": 3, "threshold": 0.5},
                timeout=5
            )
            
            if result and result.get('results'):
                # Construire contexte √† partir des r√©sultats
                knowledge_items = [
                    f"- {item['text']}"
                    for item in result['results']
                    if item.get('score', 0) > 0.5
                ]
                
                if knowledge_items:
                    knowledge_text = "\n".join(knowledge_items)
                    logger.info(f"üß† Knowledge enrichment: {len(knowledge_items)} √©l√©ments trouv√©s")
                    return knowledge_text
            
        except Exception as e:
            logger.debug(f"Pas de knowledge enrichment: {e}")
        
        return None
    
    async def _handle_learn(
        self,
        text: str,
        user_id: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        G√®re l'apprentissage de nouveaux faits dans la Knowledge Base
        
        Args:
            text: Texte √† apprendre
            user_id: ID utilisateur
            context: Contexte
            
        Returns:
            R√©sultat de l'apprentissage
        """
        logger.info(f"üìö Apprentissage d'un nouveau fait: {text}")
        
        try:
            # G√©n√©rer narration de l'action
            narration = await self._generate_action_narration(text, "learn")
            
            # Extraire le fait √† apprendre (enlever "apprends que", etc.)
            fact = re.sub(r'^(apprends?|retiens?|m√©morise|note)\s+(que\s+)?', '', text, flags=re.IGNORECASE).strip()
            
            # Appeler endpoint /learn du service LLM
            result = await self.service_registry.call_service(
                "llm",
                "/learn",
                method="POST",
                data={"text": fact},
                timeout=10
            )
            
            response_message = f"{narration} ‚Äî {result.get('total_knowledge', 0)} faits en m√©moire."
            
            logger.success(f"‚úÖ Fait appris: {fact}")
            
            return {
                "message": response_message,
                "data": result,
                "actions": ["knowledge_learn"]
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur apprentissage: {e}")
            return {
                "message": f"Je n'ai pas pu m√©moriser cette information: {str(e)}",
                "data": None,
                "actions": []
            }
    
    async def _handle_email(
        self,
        text: str,
        user_id: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """G√®re les demandes li√©es aux emails"""
        logger.info("üìß Traitement d'une demande email")
        
        try:
            # Appel au connecteur email
            result = await self.service_registry.call_service(
                "connectors",
                "/email/query",
                method="POST",
                data={"query": text, "user_id": user_id}
            )
            
            return {
                "message": result.get("message", ""),
                "data": result,
                "actions": ["email_query"]
            }
            
        except Exception as e:
            logger.error(f"Erreur email: {str(e)}")
            return {
                "message": f"Erreur d'acc√®s aux emails: {str(e)}",
                "data": None,
                "actions": []
            }
    
    async def _handle_control(
        self,
        text: str,
        user_id: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """G√®re les commandes de contr√¥le (IoT, etc.)"""
        logger.info("üè† Traitement d'une commande de contr√¥le")
        
        try:
            # Appel au connecteur IoT
            result = await self.service_registry.call_service(
                "connectors",
                "/iot/control",
                method="POST",
                data={"command": text, "user_id": user_id}
            )
            
            return {
                "message": result.get("message", ""),
                "data": result,
                "actions": ["iot_control"]
            }
            
        except Exception as e:
            logger.error(f"Erreur de contr√¥le: {str(e)}")
            return {
                "message": f"Erreur de contr√¥le: {str(e)}",
                "data": None,
                "actions": []
            }
    
    async def _handle_general(
        self,
        text: str,
        user_id: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """G√®re les requ√™tes g√©n√©rales via le LLM"""
        logger.info("üí¨ Traitement g√©n√©ral via LLM")
        
        # Similaire √† _handle_question mais avec moins de contraintes
        return await self._handle_question(text, user_id, context)
