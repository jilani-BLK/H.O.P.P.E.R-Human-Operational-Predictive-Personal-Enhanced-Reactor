# üöÄ HOPPER - Guide d'Optimisation

> Guide complet pour optimiser les performances de HOPPER : CPU, m√©moire, latence, Docker

---

## üìä Table des Mati√®res

1. [Profiling](#profiling)
2. [Optimisation Docker](#optimisation-docker)
3. [Optimisation LLM](#optimisation-llm)
4. [Optimisation STT/TTS](#optimisation-stttts)
5. [Optimisation Neo4j](#optimisation-neo4j)
6. [Optimisation R√©seau](#optimisation-r√©seau)
7. [Monitoring Continu](#monitoring-continu)

---

## üîç Profiling

### Lancer le Profiling Complet

```bash
./scripts/profile.sh
```

Ce script analyse:
- **Docker**: CPU, RAM, I/O de chaque conteneur
- **Endpoints**: Latence moyenne (5 requ√™tes)
- **M√©moire syst√®me**: Utilisation globale
- **Mod√®les**: Taille des mod√®les LLM/STT
- **Recommandations**: Bas√©es sur les m√©triques

### R√©sultats

Le rapport est sauvegard√© dans `profiling_results/profile_YYYYMMDD_HHMMSS.txt`

### Outils Avanc√©s

#### py-spy (Profiling CPU Python)

```bash
# Installation
pip install py-spy

# Profiler un processus
sudo py-spy record --duration 60 --output flamegraph.svg --pid <PID>

# Profiler toute l'ex√©cution
sudo py-spy record -o profile.svg -- python src/orchestrator/main.py
```

#### memory_profiler (Profiling M√©moire)

```bash
# Installation
pip install memory-profiler

# D√©corer les fonctions critiques
@profile
def fonction_critique():
    ...

# Profiler
python -m memory_profiler script.py
```

#### cProfile (Profiling Standard Python)

```bash
python -m cProfile -o output.pstats src/orchestrator/main.py
python -m pstats output.pstats
```

---

## üê≥ Optimisation Docker

### 1. R√©duire l'Utilisation M√©moire

#### Combiner Services L√©gers

**Avant**: 6 conteneurs s√©par√©s  
**Apr√®s**: 4 conteneurs (combiner FileSystem + LocalSystem + Spotify)

```yaml
# docker-compose.yml
services:
  hopper-connectors:
    build: ./src/connectors
    environment:
      - SERVICES=filesystem,localsystem,spotify
    ports:
      - "5006:5006"  # Multiplexer sur un seul port
```

**Gain**: -500MB RAM, -2 conteneurs

#### Limiter la M√©moire

```yaml
services:
  hopper-stt:
    deploy:
      resources:
        limits:
          memory: 2G     # Limite stricte
        reservations:
          memory: 1G     # Garantie minimale
```

### 2. Optimiser les Images

#### Utiliser Alpine Linux

**Avant**: `python:3.10` (920MB)  
**Apr√®s**: `python:3.10-alpine` (50MB)

```dockerfile
# Avant
FROM python:3.10

# Apr√®s
FROM python:3.10-alpine
RUN apk add --no-cache gcc musl-dev libffi-dev
```

**Gain**: -870MB par image

#### Multi-stage Builds

```dockerfile
# Stage 1: Builder
FROM python:3.10 as builder
WORKDIR /build
COPY requirements.txt .
RUN pip install --user -r requirements.txt

# Stage 2: Runtime
FROM python:3.10-slim
COPY --from=builder /root/.local /root/.local
COPY src/ /app/src/
ENV PATH=/root/.local/bin:$PATH
CMD ["python", "/app/src/main.py"]
```

**Gain**: -60% taille image

### 3. Cache Layers Intelligent

```dockerfile
# ‚ùå Mauvais: Invalidation cache fr√©quente
COPY . /app
RUN pip install -r requirements.txt

# ‚úÖ Bon: D√©pendances stable
COPY requirements.txt /app/
RUN pip install -r /app/requirements.txt
COPY src/ /app/src/
```

### 4. Docker Compose Optimis√©

```yaml
version: '3.8'

services:
  neo4j:
    image: neo4j:5.13
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2'
    environment:
      NEO4J_dbms_memory_heap_max__size: 1G
      NEO4J_dbms_memory_pagecache_size: 512M
    volumes:
      - ./data/neo4j:/data
    restart: unless-stopped
    
  hopper-llm:
    build:
      context: ./src/services/llm
      dockerfile: Dockerfile
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
    environment:
      MODEL_QUANTIZATION: '4bit'  # Quantization 4-bit
      TORCH_THREADS: '4'
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    restart: unless-stopped
```

---

## ü§ñ Optimisation LLM

### 1. Quantization

**R√©duction de 75% de la m√©moire sans perte significative de qualit√©**

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# Configuration 4-bit
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# Charger le mod√®le quantiz√©
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-3B",
    quantization_config=bnb_config,
    device_map="auto"
)
```

**Comparaison**:
- **Float32**: ~12GB RAM
- **Float16**: ~6GB RAM
- **8-bit**: ~3GB RAM
- **4-bit**: ~1.5GB RAM ‚úÖ

### 2. Pr√©-chargement au D√©marrage

```python
import asyncio
from functools import lru_cache

@lru_cache(maxsize=1)
def load_model():
    """Charge le mod√®le une seule fois au d√©marrage"""
    model = AutoModelForCausalLM.from_pretrained(...)
    tokenizer = AutoTokenizer.from_pretrained(...)
    return model, tokenizer

# Au d√©marrage du service
@app.on_event("startup")
async def startup_event():
    log.info("Pr√©-chargement du mod√®le LLM...")
    load_model()  # Bloque 30s au d√©marrage, mais 0s aux requ√™tes
    log.info("Mod√®le charg√© en m√©moire")
```

**Gain**: Latence premi√®re requ√™te 30s ‚Üí 0.5s

### 3. Batch Processing

```python
async def process_batch(prompts: List[str]) -> List[str]:
    """Traiter plusieurs prompts en un seul passage"""
    inputs = tokenizer(prompts, return_tensors="pt", padding=True)
    outputs = model.generate(**inputs, max_new_tokens=50)
    return tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

**Gain**: 5 requ√™tes s√©quentielles 10s ‚Üí batch 3s

### 4. Cache des R√©ponses Fr√©quentes

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def get_cached_response(prompt_hash: str) -> str:
    """Cache des r√©ponses pour prompts identiques"""
    return generate_response(prompt)

def query_llm(prompt: str) -> str:
    prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
    return get_cached_response(prompt_hash)
```

**Gain**: R√©ponses instantan√©es pour prompts r√©p√©t√©s

### 5. GPU vs CPU

```python
import torch

# D√©tecter GPU disponible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = model.to(device)

# Si GPU disponible (Metal sur macOS M1/M2)
if torch.backends.mps.is_available():
    device = torch.device("mps")
    model = model.to(device)
```

**Gain**: GPU = 10-30x plus rapide que CPU

### 6. Mod√®les Hybrides

```python
# Mod√®le rapide pour requ√™tes simples
small_model = "TinyLlama-1.1B"  # 1.1GB, 50ms latence

# Mod√®le puissant pour requ√™tes complexes
large_model = "Llama-3.2-3B"    # 6GB, 500ms latence

def select_model(prompt: str) -> str:
    if is_simple_query(prompt):  # "quelle heure?", "bonjour"
        return small_model
    else:
        return large_model
```

---

## üé§ Optimisation STT/TTS

### STT (Whisper)

#### 1. Mod√®le Optimal

**Comparaison**:
| Mod√®le | Taille | RAM | Latence | Pr√©cision |
|--------|--------|-----|---------|-----------|
| tiny   | 39MB   | 1GB | 50ms    | 70%       |
| base   | 74MB   | 1GB | 100ms   | 80%       |
| small  | 244MB  | 2GB | 300ms   | 90%       |
| medium | 769MB  | 5GB | 1s      | 95%       | ‚úÖ
| large  | 1.5GB  | 10GB| 3s      | 98%       |

**Recommandation**: `base` ou `small` pour usage quotidien

```python
import whisper

# Charger un mod√®le plus l√©ger
model = whisper.load_model("base")  # au lieu de "medium"
```

#### 2. Segments Overlappants

```python
def transcribe_with_overlap(audio_file: str) -> str:
    """Am√©liore la pr√©cision avec overlapping"""
    result = model.transcribe(
        audio_file,
        language="fr",
        task="transcribe",
        initial_prompt="Transcription en fran√ßais.",
        condition_on_previous_text=True,  # Contexte
        compression_ratio_threshold=2.4,
        logprob_threshold=-1.0
    )
    return result["text"]
```

**Gain**: +5% pr√©cision, +200ms latence

#### 3. GPU Acceleration

```python
# Utiliser GPU si disponible
model = whisper.load_model("base", device="cuda")  # ou "mps" pour M1/M2
```

**Gain**: 30x plus rapide (3s ‚Üí 100ms)

### TTS

#### 1. Cache Audio

```python
import hashlib
from pathlib import Path

CACHE_DIR = Path("/tmp/hopper_tts_cache")
CACHE_DIR.mkdir(exist_ok=True)

def synthesize_cached(text: str) -> bytes:
    """Cache des audio g√©n√©r√©s"""
    text_hash = hashlib.md5(text.encode()).hexdigest()
    cache_file = CACHE_DIR / f"{text_hash}.mp3"
    
    if cache_file.exists():
        return cache_file.read_bytes()
    
    audio = synthesize(text)
    cache_file.write_bytes(audio)
    return audio
```

**Gain**: Phrases r√©p√©t√©es instantan√©es

#### 2. Streaming Audio

```python
async def stream_audio(text: str):
    """Streamer l'audio au fur et √† mesure"""
    for chunk in split_text(text, max_length=100):
        audio_chunk = synthesize(chunk)
        yield audio_chunk
        await asyncio.sleep(0)
```

**Gain**: D√©but lecture imm√©diat (perception de rapidit√©)

---

## üìä Optimisation Neo4j

### 1. Indexation

```cypher
// Indexer les propri√©t√©s fr√©quemment requ√™t√©es
CREATE INDEX conversation_timestamp IF NOT EXISTS
FOR (c:Conversation) ON (c.timestamp);

CREATE INDEX user_name IF NOT EXISTS
FOR (u:User) ON (u.name);

CREATE INDEX entity_type IF NOT EXISTS
FOR (e:Entity) ON (e.type);
```

**Gain**: Requ√™tes 100x plus rapides

### 2. Configuration M√©moire

```properties
# neo4j.conf
dbms.memory.heap.initial_size=1G
dbms.memory.heap.max_size=2G
dbms.memory.pagecache.size=1G
```

**Recommandation**:
- Heap: 25% RAM disponible
- Pagecache: 50% RAM disponible

### 3. Nettoyage Automatique

```cypher
// Supprimer conversations >30 jours
MATCH (c:Conversation)
WHERE c.timestamp < datetime() - duration('P30D')
DETACH DELETE c;

// Archiver au lieu de supprimer
MATCH (c:Conversation)
WHERE c.timestamp < datetime() - duration('P30D')
SET c:Archived
REMOVE c:Conversation;
```

### 4. Connection Pooling

```python
from neo4j import GraphDatabase

driver = GraphDatabase.driver(
    "bolt://localhost:7687",
    auth=("neo4j", "password"),
    max_connection_pool_size=50,  # Pool de 50 connexions
    connection_acquisition_timeout=30
)
```

---

## üåê Optimisation R√©seau

### 1. Compression HTTP

```python
from fastapi import FastAPI
from fastapi.middleware.gzip import GZipMiddleware

app = FastAPI()
app.add_middleware(GZipMiddleware, minimum_size=1000)  # Compress >1KB
```

**Gain**: -70% bande passante

### 2. Cache Redis (optionnel)

```python
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379)

async def query_llm_cached(prompt: str) -> str:
    # V√©rifier cache
    cached = redis_client.get(f"llm:{prompt}")
    if cached:
        return json.loads(cached)
    
    # G√©n√©rer r√©ponse
    response = await query_llm(prompt)
    
    # Stocker en cache (expire 1h)
    redis_client.setex(f"llm:{prompt}", 3600, json.dumps(response))
    
    return response
```

### 3. HTTP/2

```python
# Utiliser Uvicorn avec HTTP/2
uvicorn.run(
    app,
    host="0.0.0.0",
    port=8000,
    http="h2"  # HTTP/2
)
```

---

## üìà Monitoring Continu

### 1. Script Monitor

```bash
# Mode temps r√©el
./scripts/monitor.sh --live

# Mode snapshot
./scripts/monitor.sh --snapshot

# Mode alertes
./scripts/monitor.sh --alert
```

### 2. Prometheus + Grafana (optionnel)

```yaml
# docker-compose.yml
services:
  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
  
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
```

### 3. Alertes Automatiques

```bash
# Alerter si CPU >80% pendant 5min
while true; do
    CPU=$(top -l 1 | grep "CPU usage" | awk '{print $3}' | sed 's/%//')
    if (( $(echo "$CPU > 80" | bc -l) )); then
        echo "ALERTE: CPU √©lev√© (${CPU}%)" | mail -s "HOPPER Alert" admin@example.com
    fi
    sleep 300
done
```

---

## üìã Checklist d'Optimisation

### Priorit√© 1 (Rapide, Impact √âlev√©)

- [ ] Activer quantization 4-bit sur LLM
- [ ] Utiliser mod√®le Whisper `base` au lieu de `medium`
- [ ] Indexer Neo4j (conversations, entit√©s)
- [ ] Activer compression HTTP (GZip)
- [ ] Limiter m√©moire Docker (deploy.resources.limits)

### Priorit√© 2 (Moyen Terme)

- [ ] Combiner services l√©gers en un conteneur
- [ ] Pr√©-charger LLM au d√©marrage
- [ ] Cache Redis pour r√©ponses fr√©quentes
- [ ] Images Alpine Linux
- [ ] Connection pooling Neo4j

### Priorit√© 3 (Long Terme)

- [ ] GPU acceleration (Whisper + LLM)
- [ ] Prometheus + Grafana monitoring
- [ ] Multi-stage Docker builds
- [ ] Mod√®les hybrides (petit + grand)
- [ ] CDN pour assets statiques

---

## üéØ R√©sultats Attendus

### Avant Optimisation

| M√©trique | Valeur |
|----------|--------|
| RAM totale | ~12GB |
| Latence LLM | 2-5s |
| Latence STT | 1-3s |
| D√©marrage | 5min |
| CPU idle | 15% |

### Apr√®s Optimisation

| M√©trique | Valeur | Gain |
|----------|--------|------|
| RAM totale | ~6GB | -50% ‚úÖ |
| Latence LLM | 0.5-1s | -75% ‚úÖ |
| Latence STT | 200-500ms | -80% ‚úÖ |
| D√©marrage | 2min | -60% ‚úÖ |
| CPU idle | 3% | -80% ‚úÖ |

---

## üîó Ressources

- [Docker Best Practices](https://docs.docker.com/develop/dev-best-practices/)
- [PyTorch Performance Tuning](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [Neo4j Performance](https://neo4j.com/developer/guide-performance-tuning/)
- [Whisper Optimization](https://github.com/openai/whisper/discussions)
- [Transformers Quantization](https://huggingface.co/docs/transformers/main_classes/quantization)

---

**Auteur**: HOPPER Team  
**Date**: Octobre 2025  
**Version**: 1.0
