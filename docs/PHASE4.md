# Phase 4 - Intelligence & Apprentissage

**Am√©lioration continue de l'intelligence de HOPPER via fine-tuning, apprentissage des pr√©f√©rences et m√©canismes de d√©cision**

---

## üìã Vue d'Ensemble

**Dur√©e**: 8 semaines (Mois 7-8)  
**Objectif**: Assistant "apprenant" adapt√© aux pr√©f√©rences utilisateur  
**Pr√©requis**: Phase 3 op√©rationnelle (Voice + Email + Notifications)

---

## üéØ Objectifs Cl√©s

1. **Fine-tuning LLM** - Adapter le mod√®le aux cas d'usage sp√©cifiques
2. **R√®gles personnalis√©es** - Configuration utilisateur (heures silence, VIP, etc.)
3. **Moteur de d√©cision** - Classification intentions, priorisation intelligente
4. **Feedback utilisateur** - Collecte satisfaction, am√©lioration continue
5. **S√©curit√© renforc√©e** - Robustesse, validation commandes, gestion erreurs

---

## üìÖ Planning D√©taill√©

### üóìÔ∏è Semaine 1-2: Collecte de Donn√©es & Infrastructure LoRA

**Objectifs:**
- Collecter conversations r√©elles Phase 3
- Identifier patterns d'erreurs/am√©liorations
- Setup infrastructure fine-tuning avec LoRA

**T√¢ches:**

1. **Logger conversations** (data/training/conversations/)
   ```python
   # src/llm_engine/conversation_logger.py
   - Enregistrer: user_input, llm_response, user_feedback, timestamp
   - Format: JSONL pour fine-tuning
   - Consentement utilisateur (opt-in)
   - Anonymisation si n√©cessaire
   ```

2. **Analyser qualit√© r√©ponses**
   ```bash
   # scripts/analyze_conversations.py
   - Identifier r√©ponses insatisfaisantes
   - Extraire patterns d'erreurs
   - G√©n√©rer rapport qualit√©
   ```

3. **Cr√©er dataset fine-tuning**
   ```python
   # data/training/finetune_dataset.jsonl
   Format: {"instruction": "...", "input": "...", "output": "..."}
   Target: 100-500 exemples qualit√©
   ```

4. **Setup LoRA infrastructure**
   ```bash
   # Installation
   pip install peft transformers bitsandbytes accelerate
   
   # Script fine-tuning
   src/llm_engine/finetune_lora.py
   - Charger mod√®le base (Mistral 7B)
   - Config LoRA (r=16, alpha=32)
   - Training 3-5 epochs
   - Sauvegarde adapters
   ```

**Livrables:**
- ‚úÖ 200+ conversations collect√©es
- ‚úÖ Dataset fine-tuning pr√™t
- ‚úÖ Script LoRA fonctionnel
- ‚úÖ Documentation process

---

### üóìÔ∏è Semaine 3-4: Fine-Tuning & √âvaluation Mod√®le

**Objectifs:**
- Entra√Æner LoRA adapters
- √âvaluer am√©lioration
- D√©ployer mod√®le affin√©

**T√¢ches:**

1. **Lancer fine-tuning**
   ```bash
   python src/llm_engine/finetune_lora.py \
     --base_model mistralai/Mistral-7B-Instruct-v0.2 \
     --dataset data/training/finetune_dataset.jsonl \
     --output_dir data/models/hopper_lora_v1 \
     --epochs 5 \
     --batch_size 4 \
     --learning_rate 2e-4
   
   # Temps estim√©: 2-4h sur Mac M1/M2
   ```

2. **√âvaluation quantitative**
   ```python
   # tests/phase4/test_llm_quality.py
   - Perplexit√© avant/apr√®s
   - Accuracy sur test set
   - Latence inf√©rence
   ```

3. **√âvaluation qualitative**
   ```bash
   # tests/phase4/manual_eval.py
   - 20 prompts test
   - Comparaison base vs fine-tuned
   - Scoring humain 1-5
   ```

4. **Int√©gration mod√®le affin√©**
   ```python
   # src/llm_engine/llm_server.py
   from peft import PeftModel
   
   base_model = AutoModelForCausalLM.from_pretrained(...)
   model = PeftModel.from_pretrained(base_model, "data/models/hopper_lora_v1")
   ```

**Livrables:**
- ‚úÖ Mod√®le LoRA entra√Æn√©
- ‚úÖ Rapport √©valuation (accuracy +10-15%)
- ‚úÖ Mod√®le d√©ploy√© dans LLM service
- ‚úÖ A/B test framework

---

### üóìÔ∏è Semaine 5: R√®gles Heuristiques & Configuration Utilisateur

**Objectifs:**
- Syst√®me de pr√©f√©rences utilisateur
- R√®gles personnalisables
- Interface configuration

**T√¢ches:**

1. **Sch√©ma configuration** (config/user_preferences.yaml)
   ```yaml
   user:
     name: "Jean"
     timezone: "Europe/Paris"
   
   notifications:
     quiet_hours:
       start: "22:00"
       end: "07:00"
     vip_contacts:
       - "marie@example.com"
       - "boss@company.com"
     min_priority: 7  # 0-10
     notification_modes:
       work_hours: "vocal"    # 9h-18h
       evening: "silent"      # 18h-22h
       night: "disabled"      # 22h-7h
   
   voice:
     activation_keyword: "hopper"
     speaker_verification: true
     voice_profile: "jean_profile_v1"
   
   email:
     auto_categorize: true
     urgent_keywords: ["urgent", "asap", "important"]
     spam_senders:
       - "newsletter@spam.com"
   
   learning:
     enable_fine_tuning: true
     feedback_frequency: "daily"  # daily, weekly
     data_retention_days: 90
   ```

2. **Moteur de r√®gles**
   ```python
   # src/orchestrator/rules_engine.py
   class RulesEngine:
       def should_notify(self, notification, time, user_prefs):
           # Check quiet hours
           # Check VIP status
           # Check priority threshold
           # Return decision + reason
       
       def categorize_email(self, email, rules):
           # Apply keyword matching
           # Sender reputation
           # Return category + confidence
   ```

3. **Interface configuration vocale**
   ```python
   # src/orchestrator/voice_config.py
   Commandes:
   - "Hopper, active le mode nuit"
   - "Hopper, ajoute Marie aux contacts VIP"
   - "Hopper, ne me d√©range pas avant 8h"
   - "Hopper, augmente la priorit√© des emails de Paul"
   ```

4. **Tests r√®gles**
   ```python
   # tests/phase4/test_rules_engine.py
   - Test quiet hours
   - Test VIP notifications
   - Test email categorization
   - Test mode switches
   ```

**Livrables:**
- ‚úÖ Syst√®me pr√©f√©rences complet
- ‚úÖ 10+ r√®gles configurables
- ‚úÖ Interface vocale config
- ‚úÖ Tests automatis√©s

---

### üóìÔ∏è Semaine 6: Moteur de D√©cision RL (Simple)

**Objectifs:**
- Classification intentions
- Priorisation adaptative
- Apprentissage par feedback

**T√¢ches:**

1. **Classifier intentions**
   ```python
   # src/orchestrator/intent_classifier.py
   from sklearn.ensemble import RandomForestClassifier
   
   class IntentClassifier:
       """
       Input: user_input, context, time, history
       Output: intent (notification_now, defer, ignore, escalate)
       """
       def train(self, examples):
           # X: features (text embeddings, time, priority)
           # y: labels (user feedback)
       
       def predict(self, notification):
           return intent, confidence
   ```

2. **Features engineering**
   ```python
   def extract_features(notification, context):
       return {
           'priority': notification.priority,
           'hour': datetime.now().hour,
           'day_of_week': datetime.now().weekday(),
           'sender_frequency': count_emails_from(sender),
           'keyword_match': has_urgent_keywords(),
           'user_activity': is_user_active(),
           'text_embedding': get_sentence_embedding(text)
       }
   ```

3. **Apprentissage par feedback**
   ```python
   # src/orchestrator/feedback_loop.py
   def collect_feedback(notification_id, user_action):
       """
       Actions: 'read_immediately', 'read_later', 'dismissed', 'marked_important'
       """
       save_to_training_data(notification_id, user_action)
       
       if len(training_data) >= 50:
           retrain_classifier()
   ```

4. **Q-Learning simple (optionnel)**
   ```python
   # src/orchestrator/rl_agent.py
   State: (notification_pending, user_activity, time_slot)
   Actions: [notify_vocal, notify_silent, defer_1h, defer_next_day]
   Reward: +1 si user satisfait, -1 si ignor√©
   
   # Update Q-table based on feedback
   ```

**Livrables:**
- ‚úÖ Intent classifier (80%+ accuracy)
- ‚úÖ Feedback loop fonctionnel
- ‚úÖ 50+ exemples training
- ‚úÖ Am√©lioration priorisation mesur√©e

---

### üóìÔ∏è Semaine 7: Feedback Utilisateur & √âvaluation Continue

**Objectifs:**
- Syst√®me d'√©valuation quotidienne
- Analyse satisfaction
- M√©triques qualit√©

**T√¢ches:**

1. **Prompt √©valuation quotidienne**
   ```python
   # src/orchestrator/daily_evaluation.py
   
   async def ask_daily_feedback():
       """
       Triggered: 20h every day
       """
       questions = [
           "Comment √©valuez-vous Hopper aujourd'hui ? (1-5)",
           "Qu'est-ce qui a bien fonctionn√© ?",
           "Qu'est-ce qui pourrait √™tre am√©lior√© ?"
       ]
       
       # Vocal ou texte
       responses = await collect_responses(questions)
       save_feedback(responses)
       analyze_sentiment(responses)
   ```

2. **Dashboard satisfaction**
   ```python
   # scripts/satisfaction_dashboard.py
   M√©triques:
   - Score moyen satisfaction (1-5)
   - √âvolution sur 30 jours
   - Top 5 probl√®mes r√©currents
   - Top 5 fonctionnalit√©s appr√©ci√©es
   - Temps r√©ponse moyen
   - Taux de commandes r√©ussies
   ```

3. **Analyse feedback texte**
   ```python
   # src/analytics/feedback_analyzer.py
   from transformers import pipeline
   
   sentiment = pipeline("sentiment-analysis")
   
   def analyze_feedback(text):
       # Sentiment: positive, negative, neutral
       # Keywords extraction
       # Topic modeling (what users talk about)
       # Identify pain points
   ```

4. **Rapport hebdomadaire automatique**
   ```python
   # scripts/weekly_report.py
   
   G√©n√®re rapport:
   - Satisfaction moyenne
   - Incidents rencontr√©s
   - Am√©liorations √† prioriser
   - Suggestions d'optimisation
   
   Envoi: email + synth√®se vocale vendredi 18h
   ```

**Livrables:**
- ‚úÖ Syst√®me feedback op√©rationnel
- ‚úÖ Dashboard satisfaction
- ‚úÖ 7 jours de donn√©es collect√©es
- ‚úÖ Premier rapport hebdomadaire

---

### üóìÔ∏è Semaine 8: S√©curit√© & Robustesse

**Objectifs:**
- Tests adversariaux
- Gestion erreurs robuste
- Validation commandes sensibles

**T√¢ches:**

1. **Tests sc√©narios d'abus**
   ```python
   # tests/phase4/test_security.py
   
   Sc√©narios:
   - Voix inconnue essaie commande syst√®me
   - Commande mal interpr√©t√©e ("supprime tous mes emails" ‚Üí risque)
   - Injection dans prompt LLM
   - D√©connexion internet pendant requ√™te
   - Service down en cascade
   - Buffer overflow audio
   - Token exhaustion LLM
   ```

2. **Validation commandes sensibles**
   ```python
   # src/orchestrator/command_validator.py
   
   DANGEROUS_COMMANDS = [
       "delete", "remove", "wipe", "format",
       "send email to all", "shutdown system"
   ]
   
   def validate_command(command, user_verified):
       if is_dangerous(command):
           if not user_verified:
               return "REJECT", "Speaker not verified"
           
           # Demander confirmation vocale
           confirm = ask_confirmation(command)
           if not confirm:
               return "REJECT", "User cancelled"
       
       return "ALLOW", None
   ```

3. **Circuit breakers**
   ```python
   # src/orchestrator/circuit_breaker.py
   
   class ServiceCircuitBreaker:
       """
       Si service √©choue 5x en 1min ‚Üí ouvert (fail fast)
       R√©essayer apr√®s 30s
       """
       def call_service(self, service_url, data):
           if self.is_open():
               raise ServiceUnavailableError()
           
           try:
               response = requests.post(service_url, json=data, timeout=5)
               self.record_success()
               return response
           except Exception as e:
               self.record_failure()
               if self.failure_threshold_reached():
                   self.open_circuit()
               raise
   ```

4. **Logs s√©curit√©**
   ```python
   # src/security/security_logger.py
   
   Log events:
   - Failed speaker verification
   - Dangerous command attempted
   - Service timeout/error
   - Unusual activity patterns
   - Data access (GDPR compliance)
   ```

5. **Tests chaos engineering**
   ```bash
   # scripts/chaos_test.sh
   
   # Kill random service
   docker-compose stop whisper
   # Check graceful degradation
   
   # Slow network
   tc qdisc add dev eth0 root netem delay 2000ms
   # Check timeout handling
   
   # Corrupt data
   echo "garbage" > data/voice_profiles/user.pkl
   # Check error recovery
   ```

**Livrables:**
- ‚úÖ 20+ tests s√©curit√©
- ‚úÖ Confirmation commandes sensibles
- ‚úÖ Circuit breakers actifs
- ‚úÖ Rapport audit s√©curit√©

---

## üèóÔ∏è Architecture Phase 4

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       HOPPER - Phase 4                           ‚îÇ
‚îÇ                  Intelligence & Apprentissage                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM Fine-Tuned  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   LoRA Adapter   ‚îÇ
‚îÇ  (Mistral 7B)    ‚îÇ        ‚îÇ  (data/models/)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Enhanced responses
         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Orchestrateur Phase 4                        ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Rules Engine ‚îÇ  ‚îÇ   Intent     ‚îÇ  ‚îÇ  Feedback Collector ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ  Classifier  ‚îÇ  ‚îÇ                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Quiet hrs  ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ - Daily eval        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - VIP list   ‚îÇ  ‚îÇ - Priority   ‚îÇ  ‚îÇ - Satisfaction      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Modes      ‚îÇ  ‚îÇ - Timing     ‚îÇ  ‚îÇ - Analytics         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ           Command Validator & Security                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Speaker verification                                ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Dangerous command confirmation                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Circuit breakers                                    ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

         ‚îÇ
         ‚îÇ Secured & Intelligent decisions
         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    User Preferences                              ‚îÇ
‚îÇ                 (config/user_preferences.yaml)                   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  - Notification rules          - Learning settings              ‚îÇ
‚îÇ  - VIP contacts                - Voice config                   ‚îÇ
‚îÇ  - Quiet hours                 - Email filters                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä M√©triques de Succ√®s

### Quantitatives

| M√©trique | Baseline (Phase 3) | Target (Phase 4) |
|----------|-------------------|------------------|
| Satisfaction utilisateur | N/A | >4.0/5.0 |
| Taux commandes r√©ussies | ~75% | >85% |
| Latence r√©ponse LLM | 2-3s | <2.5s |
| Pr√©cision intent classifier | N/A | >80% |
| Faux positifs notifications | ~30% | <15% |
| Accuracy mod√®le fine-tuned | Baseline | +10-15% |

### Qualitatives

- ‚úÖ R√©ponses plus contextuelles et personnalis√©es
- ‚úÖ Adaptation aux habitudes utilisateur
- ‚úÖ Gestion erreurs gracieuse
- ‚úÖ Confiance utilisateur accrue
- ‚úÖ R√©duction frustrations (mesur√©e par feedback)

---

## üõ†Ô∏è Stack Technique Phase 4

```python
# Fine-tuning & ML
peft==0.7.0              # LoRA adapters
transformers==4.36.0     # Hugging Face
bitsandbytes==0.41.0     # Quantization
accelerate==0.25.0       # Distributed training
scikit-learn==1.3.2      # Intent classifier
torch==2.1.0             # Deep learning

# Feedback & Analytics
pandas==2.1.4            # Data analysis
matplotlib==3.8.2        # Visualizations
seaborn==0.13.0          # Stats plots
nltk==3.8.1              # Text processing

# Security
cryptography==41.0.7     # Encryption
jwt==2.8.0               # Token validation
ratelimit==2.2.1         # API rate limiting
```

---

## üìÅ Structure Fichiers Phase 4

```
HOPPER/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ llm_engine/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ finetune_lora.py          # LoRA training script
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation_logger.py     # Log conversations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm_server.py              # (modifi√©: load LoRA)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rules_engine.py            # User rules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ intent_classifier.py       # ML intent classification
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ command_validator.py       # Security validation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feedback_loop.py           # Adaptive learning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ circuit_breaker.py         # Fault tolerance
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ daily_evaluation.py        # User feedback
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ analytics/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feedback_analyzer.py       # Sentiment analysis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ satisfaction_dashboard.py  # Metrics viz
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ security/
‚îÇ       ‚îú‚îÄ‚îÄ security_logger.py         # Audit logs
‚îÇ       ‚îî‚îÄ‚îÄ auth_validator.py          # Enhanced auth
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ user_preferences.yaml          # User config
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversations/             # Logged chats
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ finetune_dataset.jsonl     # Training data
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feedback/                  # User feedback
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îú‚îÄ‚îÄ hopper_lora_v1/            # Fine-tuned adapter
‚îÇ       ‚îî‚îÄ‚îÄ intent_classifier.pkl      # Trained classifier
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ phase4/
‚îÇ       ‚îú‚îÄ‚îÄ test_llm_quality.py        # Model evaluation
‚îÇ       ‚îú‚îÄ‚îÄ test_rules_engine.py       # Rules testing
‚îÇ       ‚îú‚îÄ‚îÄ test_intent_classifier.py  # ML testing
‚îÇ       ‚îî‚îÄ‚îÄ test_security.py           # Security tests
‚îÇ
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ analyze_conversations.py       # Data analysis
    ‚îú‚îÄ‚îÄ train_intent_classifier.py     # ML training
    ‚îú‚îÄ‚îÄ satisfaction_dashboard.py      # Metrics viz
    ‚îú‚îÄ‚îÄ weekly_report.py               # Auto reports
    ‚îî‚îÄ‚îÄ chaos_test.sh                  # Chaos engineering
```

---

## üöÄ Quick Start Phase 4

```bash
# 1. Installer d√©pendances Phase 4
pip install peft transformers bitsandbytes accelerate scikit-learn

# 2. Activer logging conversations (Phase 3 doit tourner)
python src/llm_engine/conversation_logger.py --enable

# 3. Collecter 7 jours de donn√©es
# (Utiliser normalement HOPPER pendant 1 semaine)

# 4. Analyser et cr√©er dataset
python scripts/analyze_conversations.py \
  --input data/training/conversations/ \
  --output data/training/finetune_dataset.jsonl \
  --min_quality 3

# 5. Fine-tuner mod√®le
python src/llm_engine/finetune_lora.py \
  --dataset data/training/finetune_dataset.jsonl \
  --epochs 5 \
  --output data/models/hopper_lora_v1

# 6. D√©ployer mod√®le affin√©
docker-compose restart llm

# 7. Configurer pr√©f√©rences
cp config/user_preferences.yaml.template config/user_preferences.yaml
nano config/user_preferences.yaml

# 8. Activer feedback quotidien
python src/orchestrator/daily_evaluation.py --enable

# 9. Tests s√©curit√©
pytest tests/phase4/test_security.py -v

# 10. Dashboard satisfaction
python scripts/satisfaction_dashboard.py --serve
```

---

## ‚úÖ Checklist Impl√©mentation

### Semaine 1-2: Data & LoRA
- [ ] Logger conversations activ√©
- [ ] 200+ conversations collect√©es
- [ ] Dataset fine-tuning cr√©√© (100-500 exemples)
- [ ] Script LoRA fonctionnel
- [ ] Tests sur petit dataset

### Semaine 3-4: Fine-Tuning
- [ ] Mod√®le LoRA entra√Æn√© (5 epochs)
- [ ] √âvaluation quantitative (perplexity, accuracy)
- [ ] √âvaluation qualitative (20 prompts test)
- [ ] Mod√®le d√©ploy√© dans LLM service
- [ ] Am√©lioration mesur√©e (+10-15%)

### Semaine 5: R√®gles & Config
- [ ] Schema user_preferences.yaml complet
- [ ] Rules engine impl√©ment√©
- [ ] 10+ r√®gles test√©es
- [ ] Interface vocale config
- [ ] Tests automatis√©s

### Semaine 6: Intent Classifier
- [ ] Features extracted (8+ features)
- [ ] Classifier entra√Æn√© (RandomForest/SVM)
- [ ] Accuracy >80%
- [ ] Feedback loop int√©gr√©
- [ ] 50+ exemples training

### Semaine 7: Feedback
- [ ] Daily evaluation active
- [ ] Dashboard satisfaction
- [ ] Sentiment analysis
- [ ] Rapport hebdomadaire auto
- [ ] 7 jours de feedback collect√©s

### Semaine 8: S√©curit√©
- [ ] 20+ tests s√©curit√©
- [ ] Command validator actif
- [ ] Circuit breakers impl√©ment√©s
- [ ] Logs audit s√©curit√©
- [ ] Rapport audit complet

---

## üéì Ressources

### Fine-Tuning LoRA
- [PEFT Documentation](https://huggingface.co/docs/peft)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [QLoRA: Efficient Fine-tuning](https://arxiv.org/abs/2305.14314)

### Intent Classification
- [Scikit-learn Classifiers](https://scikit-learn.org/stable/supervised_learning.html)
- [RASA NLU](https://rasa.com/docs/rasa/nlu-training-data/)

### Reinforcement Learning (Optionnel)
- [Stable Baselines3](https://stable-baselines3.readthedocs.io/)
- [OpenAI Spinning Up](https://spinningup.openai.com/)

### Security
- [OWASP AI Security](https://owasp.org/www-project-ai-security-and-privacy-guide/)
- [Prompt Injection Defense](https://simonwillison.net/2023/Apr/14/worst-that-can-happen/)

---

## üìù Notes Importantes

### Consentement & RGPD
```python
# Toujours demander consentement explicite
def request_data_consent():
    """
    "Hopper souhaite apprendre de vos conversations pour mieux vous servir.
    Acceptez-vous que vos interactions soient enregistr√©es de mani√®re anonyme ?
    Vous pouvez refuser ou retirer votre consentement √† tout moment."
    """
    consent = await get_user_response()
    save_consent(user_id, consent, timestamp)
    return consent
```

### Fine-Tuning Mac Optimisations
```python
# Utiliser quantization 4-bit
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    device_map="auto"
)

# LoRA config pour Mac
lora_config = LoraConfig(
    r=16,              # Rank (plus petit = moins de VRAM)
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
```

### Temps Estim√©s (Mac M1/M2)
- **Data collection**: 7 jours usage normal
- **Dataset preparation**: 2-4 heures
- **Fine-tuning LoRA**: 2-4 heures (500 samples, 5 epochs)
- **Evaluation**: 30 minutes
- **Full Phase 4**: 8 semaines

---

**Version**: 4.0.0  
**Status**: PR√äT √Ä D√âMARRER (apr√®s Phase 3 op√©rationnelle)  
**Derni√®re mise √† jour**: 5 novembre 2025
