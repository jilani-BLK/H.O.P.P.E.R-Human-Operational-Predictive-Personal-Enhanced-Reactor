# HOPPER - Optimisations Performance (Quick Wins)

**Date**: 22 Octobre 2025  
**Objectif**: AmÃ©liorer latence LLM de 40% (11s â†’ 6-7s)

## ðŸŽ¯ Optimisations Ã  Appliquer

### 1. Augmenter GPU Layers (PRIORITÃ‰ HAUTE)

**Changement**:
```bash
# .env
LLM_N_GPU_LAYERS=1  â†’  LLM_N_GPU_LAYERS=10
```

**Impact attendu**:
- Latence: -30-40%
- Utilisation GPU Metal: Optimale pour M3 Max
- StabilitÃ©: Ã€ tester (actuellement 1 layer = trÃ¨s stable)

**Commandes**:
```bash
# 1. Backup configuration actuelle
cp .env .env.backup

# 2. Modifier GPU layers
sed -i '' 's/LLM_N_GPU_LAYERS=1/LLM_N_GPU_LAYERS=10/' .env

# 3. RedÃ©marrer service LLM
docker compose restart llm

# 4. Attendre chargement modÃ¨le (~30s)
sleep 30

# 5. Tester performance
time curl -s -X POST http://localhost:8000/command \
  -H "Content-Type: application/json" \
  -d '{"text": "Explique Python en 2 phrases"}'
```

---

### 2. RÃ©duire Context Window (PRIORITÃ‰ MOYENNE)

**Changement**:
```bash
# .env
LLM_CONTEXT_SIZE=4096  â†’  LLM_CONTEXT_SIZE=2048
```

**Impact attendu**:
- Latence: -10-15%
- MÃ©moire: -500 MB RAM
- Historique conversation: 10 â†’ 5 Ã©changes (acceptable)

**Commandes**:
```bash
# Modifier context size
sed -i '' 's/LLM_CONTEXT_SIZE=4096/LLM_CONTEXT_SIZE=2048/' .env

# RedÃ©marrer
docker compose restart llm
```

---

### 3. Truncation Historique (PRIORITÃ‰ MOYENNE)

**Changement**:
```python
# src/orchestrator/core/prompt_builder.py
max_history_tokens=2048  â†’  max_history_tokens=1024
```

**Impact attendu**:
- Taille prompts: -15-20%
- Latence: -5-10%

---

### 4. Cache Embeddings KB (PRIORITÃ‰ BASSE)

**Code Ã  ajouter**:
```python
# src/llm_engine/knowledge_base.py
from functools import lru_cache

@lru_cache(maxsize=128)
def _encode_cached(self, text: str):
    return self.encoder.encode([text])[0]
```

**Impact attendu**:
- Recherche KB: 50ms â†’ <10ms
- MÃ©moire: +10 MB

---

## ðŸ“Š Plan d'ExÃ©cution

**Phase 1 - ImmÃ©diat (5 minutes)**:
1. âœ… GPU Layers: 1 â†’ 10
2. âœ… RedÃ©marrage LLM service
3. âœ… Test latence

**Phase 2 - Optionnel (10 minutes)**:
4. Context Window: 4096 â†’ 2048
5. Test stabilitÃ©
6. Mesure amÃ©lioration

**Validation**:
```bash
# Benchmark avant optimisation
# Moyenne: 11s pour 55 tokens

# Benchmark aprÃ¨s optimisation
# Objectif: <7s pour 55 tokens
```

---

## ðŸ”„ Rollback si ProblÃ¨me

```bash
# Restaurer configuration d'origine
cp .env.backup .env
docker compose restart llm

# VÃ©rifier santÃ©
curl http://localhost:5001/health
```

---

**Gain total attendu**: Latence 11s â†’ **6-7s** (~40% amÃ©lioration)
