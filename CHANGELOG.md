# Changelog

Toutes les modifications notables de ce projet seront document√©es dans ce fichier.

Le format est bas√© sur [Keep a Changelog](https://keepachangelog.com/fr/1.0.0/),
et ce projet adh√®re √† [Semantic Versioning](https://semver.org/lang/fr/).

## [Non publi√©]

### √Ä venir (Phase 3)
- Am√©lioration routing (75% ‚Üí 90%+ pr√©cision)
- D√©monstration compl√®te RAG avec Knowledge Base
- Commande "hopper learn" pour apprentissage interactif
- Optimisation performance (<1s conversations courtes)
- Streaming r√©ponses LLM token-par-token
- PermissionManager + ConsentPolicy (SQLite)
- AuditStore pour tra√ßabilit√© compl√®te
- ActionNarrator + TTS feedback
- Knowledge Graph Neo4j integration
- GPU acceleration pour LLM (Metal macOS)
- Cache de plans LLM (Redis)
- Tests BDD avec Gherkin/Behave
- Interface graphique (dashboard)

## [0.2.1-phase2] - 2025-11-04

### üéâ Phase 2 VALID√âE - Conversations LLM (MAJEUR)

#### ‚úÖ Validation Officielle
- **Tests automatis√©s**: 15/20 r√©ussis (75%, crit√®re ‚â•70% atteint)
- **Performance**: 810ms moyenne (cible <5s atteinte)
- **Mode offline**: 100% via Ollama v0.12.6
- **Conversations fran√ßaises**: Naturelles avec llama3.2 (2GB)
- **Multi-tour**: Contexte maintenu sur 10 messages

#### Nouveaux Composants (1075+ lignes)

**LLMDispatcher** (`src/orchestrator/core/llm_dispatcher.py`, 190 lignes):
- Routage automatique syst√®me vs conversation
- Templates de prompts avec personnalit√© HOPPER
- D√©tection contextuelle d'intentions (mots-cl√©s + patterns)
- Int√©gration API LLM service
- Gestion historique et contexte

**Phase2 Routes** (`src/orchestrator/api/phase2_routes.py`, 212 lignes):
- Endpoint unifi√© `POST /api/v1/command`
- Support dual: commandes syst√®me + conversations
- Mod√®les Pydantic: CommandRequest, CommandResponse
- Health checks d√©taill√©s: `/api/v1/status`
- Format JSON structur√© (type, action/response, dur√©e, tokens)

**Orchestrateur Phase 2** (`src/orchestrator/main_phase2.py`, 75 lignes):
- FastAPI app avec phase2_routes
- CORS configur√©
- Startup/shutdown events
- Logging structur√©

**ConversationManager** (`src/orchestrator/core/conversation_manager.py`, 200 lignes):
- Gestion historique conversations en m√©moire
- Dataclasses Message/Conversation
- Limite 10 messages (gestion tokens)
- Truncation automatique
- Thread-safe storage

**CLI v2 Interactif** (`hopper_cli_v2.py`, 178 lignes):
- Mode REPL avec prompt `hopper>`
- Mode single-command pour questions/commandes ponctuelles
- Commandes sp√©ciales: `clear`, `help`, `exit`
- Affichage enrichi: emoji, dur√©e, tokens
- Historique session complet

**Tests Validation** (`scripts/test/validate_phase2.py`, 220 lignes):
- 20 tests automatis√©s (12 conversations + 8 syst√®me)
- Validation type (syst√®me/conversation)
- V√©rification mots-cl√©s dans r√©ponses
- Mesure latence par test
- Rapport d√©taill√© avec statistiques

#### Int√©gration LLM

**Ollama + llama3.2**:
- Version Ollama: v0.12.6
- Mod√®le actif: llama3.2:latest (2GB)
- Mod√®les disponibles: llama2, mistral, llama3.1:8b, llama3.2
- Configuration Docker: host.docker.internal:11434
- Performance: 30-50 tokens/seconde

**Knowledge Base FAISS**:
- 25 documents charg√©s
- Vector Store: IndexFlatIP (similarit√© cosine)
- Embeddings: all-MiniLM-L6-v2 (384 dimensions)
- Recherche: <50ms par requ√™te
- Statut: Infrastructure pr√™te (RAG √† tester Phase 3)

#### M√©triques Phase 2

**Performance**:
- Latence syst√®me: 25ms moyenne (min=4ms, max=28ms)
- Latence conversation: 1529ms moyenne (min=342ms, max=2849ms)
- Latence globale: 810ms moyenne
- Tokens par √©change: 250-310 (prompt ~150, r√©ponse 100-160)

**Tests**:
- Syst√®me: 6/8 r√©ussis (75%)
- Conversation: 9/12 r√©ussis (75%)
- Total: 15/20 r√©ussis (75%)

**Qualit√©**:
- Fran√ßais naturel: 100% r√©ponses correctes
- Pertinence: 75% r√©ponses pertinentes
- Coh√©rence persona: HOPPER maintient son identit√©
- Contexte multi-tour: R√©f√©rences bien comprises

#### Modifications Configuration

**docker-compose.yml**:
- Variables Ollama: `OLLAMA_HOST=http://host.docker.internal:11434`
- Mod√®le: `OLLAMA_MODEL=llama3.2`
- Contexte: `LLM_CONTEXT_SIZE=4096`

**docker/orchestrator.Dockerfile**:
- CMD chang√© vers `main_phase2.py`
- Commentaires phases (Phase 1, 2, 3+)

#### Documentation

- `PHASE2_VALIDATION.md`: R√©sultats tests d√©taill√©s
- `PHASE2_FINAL_REPORT.md`: Rapport complet 15 pages
- `PHASE2_QUICK_REF.md`: Guide rapide utilisation
- `README.md`: Section Phase 2 mise √† jour

#### Probl√®mes Connus

**Routing (3 √©checs)**:
- "Quel mod√®le utilises-tu?" ‚Üí Mal rout√© vers syst√®me
- "√Ä quoi servent les fichiers?" ‚Üí Mal rout√© vers syst√®me
- "ls /tmp" ‚Üí Mal rout√© vers conversation
- Solution Phase 3: Classification LLM des intentions

**Limitation Docker**:
- "ouvre Calculator" ‚Üí √âchec (pas de GUI dans conteneur)
- Comportement attendu et document√©

## [0.2.0-alpha] - 2025-11-01

### üöÄ Architecture LLM-First (MAJEUR)

#### Nouveaux Composants Core (1730+ lignes)
- **Models Layer** (`core/models.py`, 430 lignes): Pydantic schemas complets
  - `InteractionEnvelope`: Normalisation tous inputs (voix, texte, √©v√©nements)
  - `SystemPlan`: Plan d'action g√©n√©r√© par LLM
  - `ToolCall`: Appels d'outils structur√©s
  - `PerceptionEvent`, `ConsentPolicy`, `AuditEntry`
  - Enums: `RiskLevel`, `ToolStatus`, `ConsentMode`, `InteractionType`

- **PromptAssembler** (`core/prompt_assembler.py`, 400 lignes): Injection contextuelle
  - Historique conversation complet
  - R√©sultats RAG (top-k=3 via FAISS)
  - Permissions actives et audit r√©cent
  - Function calling schema pour LLM
  - Reformulation avec r√©sultats d'ex√©cution

- **LlmAgent** (`core/llm_agent.py`, 550 lignes): Pipeline ReAct complet
  - **THOUGHT**: Assembler contexte via PromptAssembler
  - **ACT**: LLM g√©n√®re SystemPlan JSON valid√© Pydantic
  - **OBSERVE**: Ex√©cution s√©quentielle d'outils avec permissions
  - **ANSWER**: Reformulation naturelle avec r√©sultats
  - JSON parsing robuste avec `JSONDecoder.raw_decode()`
  - Timeout fallback + retry logic

- **PerceptionBus** (`core/perception_bus.py`, 350 lignes): Event-driven architecture
  - Pub/Sub asyncio avec `asyncio.Queue`
  - Subscribe par type ou wildcard
  - Historique 100 derniers √©v√©nements
  - Stats par source et type

- **LlmFirstDispatcher** (`core/llm_first_dispatcher.py`): Dispatcher intelligent
  - Remplace regex hardcod√©es par planning LLM
  - Fallback heuristique si LLM √©choue
  - M√©triques succ√®s/√©chec + timing

- **ToolExecutor** (`core/tool_executor.py`): Unified tool execution
  - 6 outils support√©s: system_executor, llm_knowledge, email, calendar, tts, stt
  - Timeout configurable par outil
  - Pr√©paration automatique des payloads
  - Logging d√©taill√© + error handling

#### System Executor C - Refactorisation Majeure
- ‚úÖ **JSON Parsing Complet** avec cJSON
  - Parsing requ√™tes POST avec body accumulation
  - Extraction `action`, `path`, `content` du JSON
  - Switch sur actions: create_file, delete_file, list_directory

- ‚úÖ **Nouvelles Fonctions**
  - `create_file_with_content(path, content)`: Cr√©ation avec contenu custom
  - `delete_file(path)`: Suppression avec `remove()`
  - `list_directory(path)`: Listage avec `opendir()` + `readdir()`
  - JSON responses structur√©es

- ‚úÖ **Performance**: <10ms par action (vs 50-100ms Python)

#### Bugs Critiques R√©solus
- ‚úÖ **CLI Broken** (P0): D√©pendance `requests` manquante ‚Üí install√©e
- ‚úÖ **Context Manager** (P0): Historique non sauvegard√© ‚Üí `append(exchange)` ajout√©
- ‚úÖ **System Executor** (P0): Stub hardcod√© ‚Üí JSON parsing complet

### Tests d'Int√©gration
- ‚úÖ Test suite compl√®te (`tests/test_llm_first_integration.py`)
- ‚úÖ Tests end-to-end: create_file, list_directory, delete_file
- ‚úÖ Test multi-tour conversation avec contexte
- ‚úÖ Test error handling + JSON parsing robustness
- ‚úÖ Tests direct system_executor C

### Documentation
- ‚úÖ `LLM_FIRST_SUCCESS.md`: Rapport succ√®s complet (validation, tests, m√©triques)
- ‚úÖ `QUICKSTART_LLM_FIRST.md`: Guide d√©marrage rapide
- ‚úÖ `ARCHITECTURE_LLM_FIRST.md`: Documentation architecture technique
- ‚úÖ `ETAT_REEL.md`: √âtat honn√™te projet avant transformation
- ‚úÖ `CORRECTIFS_APPLIQUES.md`: Documentation bugs critiques

### Am√©liorations Performance
- LLM planning: 15-20s (acceptable pour CPU Mistral-7B)
- System Executor C: <10ms par action
- Total pipeline: ~18-20s user input ‚Üí final response

### √Ä Venir (Prochaine Version)
- PermissionManager + SQLite
- AuditStore + SQLite
- Tests BDD (Gherkin/Behave)
- GPU acceleration LLM (‚Üí 2-3s au lieu de 15s)
- Cache plans LLM (Redis)

## [0.1.0-alpha] - 2025-10-22

### Ajout√©

#### Architecture Microservices
- Architecture compl√®te en 7 microservices Docker
- Communication inter-services via REST HTTP/JSON
- Isolation compl√®te via conteneurs Docker
- Orchestration avec Docker Compose

#### Orchestrateur Central (Python)
- API REST compl√®te avec FastAPI
- Syst√®me de routage d'intentions (IntentDispatcher)
- Gestion du contexte conversationnel (ContextManager)
- Registre de services avec health checks (ServiceRegistry)
- Support de 50 derniers √©changes en m√©moire
- Gestion des timeouts et retry logic

#### Module d'Ex√©cution Syst√®me (C)
- Serveur HTTP l√©ger avec libmicrohttpd
- Actions syst√®me: cr√©ation/suppression fichiers
- Listage de r√©pertoires
- Lancement d'applications macOS
- Logging structur√©
- R√©ponses JSON avec cJSON

#### Moteur LLM (Python/C++)
- Support llama.cpp pour inf√©rence optimis√©e
- Mode simulation (sans mod√®le) pour tests
- API de g√©n√©ration de texte
- Support du contexte enrichi
- Pr√™t pour GPU Apple Silicon (Metal)
- Support mod√®les LLaMA 2 / Mistral

#### Module STT (Python)
- Int√©gration OpenAI Whisper
- Support multilingue (fran√ßais prioritaire)
- API de transcription de fichiers audio
- Mode streaming pr√©vu (Phase 2)

#### Module TTS (Python)
- Synth√®se vocale avec support macOS
- API de g√©n√©ration audio
- Support voix fran√ßaises
- Latence optimis√©e

#### Module d'Authentification (Python)
- API de v√©rification vocale
- API de v√©rification faciale (skeleton)
- Syst√®me d'enregistrement utilisateur
- Pr√™t pour SpeechBrain/Resemblyzer

#### Module Connecteurs (Python)
- Structure pour connecteurs externes
- Squelettes Email, IoT, Calendrier
- API unifi√©e pour int√©grations

#### CLI (Python)
- Mode interactif complet
- Mode commande directe
- Commandes syst√®me (/health, /clear, /help)
- Formatage color√© des sorties
- Gestion d'erreurs √©l√©gante
- Support des alias

#### Infrastructure
- Script d'installation automatis√© (install.sh)
- Configuration via fichier .env
- Makefile avec 25+ commandes utiles
- Fichiers .gitignore complets
- Structure de dossiers compl√®te

#### Documentation
- README.md principal (guide complet)
- ARCHITECTURE.md (60+ pages techniques)
- QUICKSTART.md (installation rapide)
- DEVELOPMENT.md (guide d√©veloppeur)
- CONTRIBUTING.md (guide de contribution)
- STRUCTURE.md (visualisation arborescence)
- PROJECT_SUMMARY.md (r√©sum√©)
- CHANGELOG.md (ce fichier)

#### Tests
- Tests d'int√©gration de base
- Structure de tests unitaires
- Commandes make pour testing

### S√©curit√©
- Isolation des services via Docker
- Logging de toutes les actions
- Validation des entr√©es
- Support authentification multi-facteurs (pr√©vu)
- Aucune d√©pendance cloud

### Performances
- Code C pour actions critiques
- C++ pour inf√©rence LLM
- Support GPU Apple Silicon
- Latence <100ms pour actions syst√®me
- Pool de connexions HTTP r√©utilisables

## Notes de Version

### [0.1.0-alpha] - Phase 1 Compl√©t√©e

Cette version alpha marque l'ach√®vement de la **Phase 1** du projet HOPPER:

**‚úÖ Accomplissements**:
- Architecture microservices compl√®te et fonctionnelle
- 7 services dockeris√©s et orchestr√©s
- >3000 lignes de code (Python, C, C++)
- 100+ pages de documentation
- CLI interactif op√©rationnel
- Infrastructure pr√™te pour Phase 2

**‚ö†Ô∏è Limitations Connues**:
- LLM en mode simulation (mod√®le non inclus)
- STT/TTS en mode basique
- Authentification en mode skeleton
- Connecteurs non impl√©ment√©s (Phase 2)
- Pas d'interface graphique (Phase 5)

**üéØ Prochaine Version (0.2.0)**:
- Int√©gration mod√®le LLM r√©el
- Connecteur Email fonctionnel
- Interface vocale compl√®te (STT+TTS)
- Connecteur IoT de base
- Tests d'int√©gration complets

### Compatibilit√©

- **Syst√®mes**: macOS M1/M2/M3, Linux x86_64
- **Docker**: >= 20.10
- **Python**: >= 3.10
- **Compilateur C**: gcc >= 12.0 ou clang >= 14.0

### Installation

```bash
git clone https://github.com/jilani-BLK/H.O.P.P.E.R-Human-Operational-Predictive-Personal-Enhanced-Reactor.git
cd HOPPER
./install.sh
```

### Migration

Premi√®re version - Pas de migration n√©cessaire.

---

**L√©gende**:
- `Ajout√©` : Nouvelles fonctionnalit√©s
- `Modifi√©` : Changements de fonctionnalit√©s existantes
- `D√©pr√©ci√©` : Fonctionnalit√©s bient√¥t supprim√©es
- `Supprim√©` : Fonctionnalit√©s supprim√©es
- `Corrig√©` : Corrections de bugs
- `S√©curit√©` : Vuln√©rabilit√©s corrig√©es

---

*Derni√®re mise √† jour: 22 octobre 2025*
